{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CS-C3240 - Machine Learning D**\n",
    "**Project**\n",
    "\n",
    "**Authors: Aaron Gutierrez-Hernandez & Alexandre Cojot**\n",
    "\n",
    "**Date created: 10-sep-2023**\n",
    "\n",
    "**Last modified: 22-sep-2023**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Read the .csv file containing the dataset\n",
    "data = pd.read_csv('heart_data.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info() # check basic dataframe's information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.duplicated().sum() # look if there are duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop_duplicates(inplace=True) # drop druplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe() # basic descriptive statistics of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,9)) \n",
    "sns.heatmap(data.corr(),annot=True) # see correlation between features and features with response variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,9))\n",
    "sns.pairplot(data) # see distribution between each pair of features and features with response variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at how the response variable is splitted to avoid fitting the model to just one value\n",
    "target_counts = data['target'].value_counts() # count response variable values to see if they are not unbalanced\n",
    "target_ratios = target_counts/len(data)       # get the same information in a ratios format\n",
    "print(target_counts)\n",
    "print(50*'_')\n",
    "print(target_ratios)\n",
    "sns.countplot(x=data['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make categorical data to the corresponding features\n",
    "categorical_features = ['sex','cp','fbs','restecg','exang','thal','target'] # columns containing categorical features \n",
    "data[categorical_features] = data[categorical_features].astype('category')  # converting dtype to categorical\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize numerical features\n",
    "numeric_features = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak', 'slope', 'ca'] # columns containing numerical features \n",
    "scaler = StandardScaler()          # initialize scaler\n",
    "scaler.fit(data[numeric_features]) # fit the scaler to the selected numeric columns\n",
    "data[numeric_features] = scaler.transform(data[numeric_features]) # standardize the numeric columns\n",
    "data.describe() # basic descriptive statistics of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('target',axis=1) # split the features from the labels\n",
    "y = data['target']             # split the labels from the features\n",
    "print(X.shape,y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs = ([LogisticRegression(solver='liblinear', penalty='l2'),\n",
    "         RandomForestClassifier(criterion='entropy',max_depth=5)]) # initialize proposed classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf   = StratifiedKFold(n_splits=5, shuffle=True, random_state=0) # initialize k-folds\n",
    "acc_valid = []                                                   # store validation accuracy\n",
    "for clf in clfs:\n",
    "    warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "    scrs = cross_val_score(clf, X, y, cv=kf, scoring='accuracy') # compute k-fold cross validation scores\n",
    "    acc_valid.append(scrs.mean())\n",
    "    print(70*'_')                                                \n",
    "    print(clf)\n",
    "    print(f'Scores: {scrs}')\n",
    "    print(f'Scores mean: {scrs.mean():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.2,\n",
    "                                                    stratify=y, random_state=0) # split data in training and testing sets\n",
    "y_sets = {'y_train': y_train, 'y_test': y_test}\n",
    "\n",
    "for y_name, y_set in y_sets.items():\n",
    "    y_counts = y_set.value_counts() # count response variable values to see if they are not unbalanced\n",
    "    y_ratios = y_counts/len(y_set)  # get the same information in a ratios format\n",
    "    print(50*'_')\n",
    "    print(y_name)\n",
    "    print('Set sizes')\n",
    "    print(y_set.shape,y_set.shape)\n",
    "    print('Label counts')\n",
    "    print(y_counts)\n",
    "    print('Label ratios')\n",
    "    print(y_ratios)\n",
    "    sns.countplot(x=y_set)\n",
    "    plt.title(str(y_name)+' label counts')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_confmat(y_true, y_pred, title, ax):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', square=True, cbar= False,\n",
    "                xticklabels=['No Heart Disease', 'Heart Disease'],\n",
    "                yticklabels=['No Heart Disease', 'Heart Disease'],\n",
    "                ax=ax)\n",
    "    ax.set_xlabel('Predicted Labels')\n",
    "    ax.set_ylabel('True Labels')\n",
    "    ax.set_title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train = []\n",
    "acc_test  = []\n",
    "for clf in clfs:\n",
    "    clf.fit(X_train, y_train)         # train classifier (fit to training data)  \n",
    "    y_pred = clf.predict(X_test)      # compute the predictions (use the trained model on test data) \n",
    "    train_pred = clf.predict(X_train) # compute the predictions (use the trained model on train data) \n",
    "    # Train and test scores\n",
    "    train_score = accuracy_score(y_train, train_pred)\n",
    "    test_score  = accuracy_score(y_test, y_pred)\n",
    "    acc_train.append(train_score)\n",
    "    acc_test.append(test_score)\n",
    "    # Model evaluation metrics\n",
    "    print(60*'_')                                                \n",
    "    print(clf)\n",
    "    print(f'Train Accuracy: {100*train_score:.2f}','%')\n",
    "    print(f'Test Accuracy : {100*test_score:.2f}','%')\n",
    "    print('Report:\\n',classification_report(y_test, y_pred))\n",
    "    # Visualize Confusion Matrices\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12,6))\n",
    "    plt_confmat(y_train, train_pred, 'Train Confusion Matrix', axes[0])\n",
    "    plt_confmat(y_test, y_pred, 'Test Confusion Matrix', axes[1])\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subset scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['Logistic Regression', 'Random Forest'])\n",
    "df.loc[0] = acc_train\n",
    "df.loc[1] = acc_valid\n",
    "df.loc[2] = acc_test\n",
    "df.loc[3] = [1-acc for acc in acc_train]\n",
    "df.loc[4] = [1-acc for acc in acc_valid]\n",
    "df.loc[5] = [1-acc for acc in acc_test]\n",
    "df.loc[6] = df.iloc[0:3].mean()\n",
    "df.loc[7] = df.iloc[3:6].mean()\n",
    "rows = {\n",
    "    0: 'Training Accuracy',\n",
    "    1: 'Validation Accuracy',\n",
    "    2: 'Test Accuracy',\n",
    "    3: 'Training Error',\n",
    "    4: 'Validation Error',\n",
    "    5: 'Test Error',\n",
    "    6: 'Average Accuracy',\n",
    "    7: 'Average Error'\n",
    "}\n",
    "df = df.rename(index=rows)\n",
    "df = df.round(4)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,4.5))\n",
    "df[0:3].plot(kind='bar',rot=0,alpha=.6)\n",
    "df[3:6].plot(kind='bar',rot=0,ax=plt.gca(), alpha=1)\n",
    "plt.legend(title='Classifiers', loc='upper left', bbox_to_anchor=(1, 1),\n",
    "           labels=['Log Regression Accuracy','Random Forest Accuracy',\n",
    "                   'Log Regression Error   ','Random Forest Error'])\n",
    "plt.xlabel('Data subsets')\n",
    "plt.xticks([0,1,2],['Training','Validation','Training'])\n",
    "plt.ylabel('Accuracy/Error')\n",
    "plt.title('Accuracy and Error for data subsets')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
